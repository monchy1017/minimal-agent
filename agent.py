import os
from typing import TypedDict, List, Optional
from dotenv import load_dotenv
import arxiv
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END, START
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

load_dotenv()
if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("OPENAI_API_KEYãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")


@tool
def web_search(query: str) -> str:
    """
    è«–æ–‡å†…ã®ä¸æ˜ãªå°‚é–€ç”¨èªã‚„ã€GitHubã®å®Ÿè£…ãƒªãƒã‚¸ãƒˆãƒªã‚’æ¢ã™ãŸã‚ã«Webæ¤œç´¢ã‚’è¡Œã†
    Googleæ¤œç´¢ã®ä»£ã‚ã‚Šã«ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print(f"\n  ğŸ” [Toolä½¿ç”¨] Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­: '{query}'")
    try:
        search = DuckDuckGoSearchRun()
        result = search.invoke(query)
        # çµæœã®é•·ã•ã‚‚è¡¨ç¤ºã—ã¦ã¿ã‚‹
        print(f"     -> çµæœå–å¾—æˆåŠŸ ({len(result)}æ–‡å­—)")
        return result
    except Exception as e:
        print(f"     -> æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"


class PaperInfo(TypedDict):
    title: str
    summary: str
    url: str


class AgentState(TypedDict):
    keyword: str
    queries: List[str]  # [NEW] ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ
    core_papers: Optional[List[PaperInfo]]
    analysis: Optional[List[str]]
    report_markdown: Optional[str]


def generate_queries(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰0: [NEW] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’arXivç”¨ã®è‹±èªã‚¯ã‚¨ãƒªã«å¤‰æ›ã™ã‚‹"""
    print("generating search queries...")
    keyword = state["keyword"]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = f"""
    You are an expert AI researcher.
    Please generate 3 effective search queries for arXiv based on the user's input topic.
    
    Requirements:
    1. Translate to English if necessary.
    2. Use specific academic terms (e.g., "LLM efficiency" -> "Model Compression", "Quantization").
    3. Output ONLY the 3 queries, separated by newlines. No numbering or bullets.
    
    User Input: "{keyword}"
    """

    response = llm.invoke([HumanMessage(content=prompt)])

    # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–ã—ã€ç©ºç™½ã‚’é™¤å»
    queries = [q.strip() for q in response.content.split("\n") if q.strip()]

    print(f"Generated Queries: {queries}")
    return {**state, "queries": queries}


def find_core_papers(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰1: ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ä½¿ã£ã¦è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹"""
    print("finding core papers...")

    # ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªãŒãªã„å ´åˆã¯å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    queries = state.get("queries", [state["keyword"]])

    client = arxiv.Client()
    all_papers: List[PaperInfo] = []
    seen_urls = set()

    # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œ
    for query in queries:
        print(f"  Search arXiv with: '{query}'")
        search = arxiv.Search(
            query=query,
            max_results=3,
            sort_by=arxiv.SortCriterion.Relevance,
        )

        try:
            for result in client.results(search):
                if result.pdf_url not in seen_urls:
                    paper = PaperInfo(
                        title=result.title,
                        summary=result.summary,
                        url=result.pdf_url,
                    )
                    all_papers.append(paper)
                    seen_urls.add(result.pdf_url)
        except Exception as e:
            print(f"  Error searching for '{query}': {e}")
    final_papers = all_papers[:5]

    if not final_papers:
        raise ValueError(f"è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Queries: {queries}")

    print(f"Total {len(final_papers)} unique papers found.")
    return {**state, "core_papers": final_papers}


def analyze_paper_with_llm(state: AgentState) -> AgentState:
    """
    ãƒãƒ¼ãƒ‰2: LLMã§è«–æ–‡ã‚’åˆ†æã™ã‚‹
    å¿…è¦ãªã‚‰æ¤œç´¢ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
    """
    print("analyzing papers with llm...")
    papers = state.get("core_papers")
    if not papers:
        return state
    tools = [web_search]
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0).bind_tools(tools)
    agent_executor = create_react_agent(llm, tools)
    analysis_results: List[str] = []

    for i, paper in enumerate(papers):
        print(f"Analyzing ({i+1}/{len(papers)}): {paper['title'][:30]}...")
        prompt = f"""
        You are a thorough researcher.
        Read the abstract below and summarize the "Core Contribution" in Japanese (about 200 characters).
        
        If the summary contains â€œunknown technical termsâ€ or â€œimplementation statusâ€
        requiring additional information, please use the provided [web_search] tool to investigate.
        Once sufficient information has been gathered, please output the final explanation.
            
        Title: {paper['title']}
        Abstract:
        {paper['summary']}
        
        Core Contribution (Japanese):
        """
        try:
            result = agent_executor.invoke(
                {"messages": [HumanMessage(content=prompt)]}
            )
            messages = result["messages"]
            for msg in messages:
                # LLMãŒã€Œãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã„ãŸã„ã€ã¨è¨€ã£ãŸæ™‚
                if isinstance(msg, AIMessage) and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        print(
                            f"    ğŸ¤– [AIMsg] {tool_call['name']} ã‚’ä½¿ãŠã†ã¨ã—ã¦ã„ã¾ã™..."
                        )
                        print(f"       å¼•æ•°: {tool_call['args']}")

                # ãƒ„ãƒ¼ãƒ«ãŒå®Ÿè¡Œã•ã‚Œã¦çµæœãŒè¿”ã£ã¦ããŸæ™‚
                elif isinstance(msg, ToolMessage):
                    print(
                        f"    ğŸ“¦ [ToolMsg] ãƒ„ãƒ¼ãƒ«ã‹ã‚‰çµæœãŒè¿”ã£ã¦ãã¾ã—ãŸ (å†’é ­200æ–‡å­—): {msg.content[:200]}..."
                    )

            final_response = messages[-1].content
            analysis_results.append(final_response)
        except Exception as e:
            print(f"Error analyzing paper {i}: {e}")
            analysis_results.append("åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    return {**state, "analysis": analysis_results}


def compile_report(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰3: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹"""
    print("compiling report...")
    papers = state.get("core_papers")
    analyses = state.get("analysis")
    keyword = state["keyword"]
    queries = state.get("queries", [])

    if not papers or not analyses:
        return {**state, "report_markdown": "æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}

    report = f"# è«–æ–‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n"
    report += f"**å…¥åŠ›ãƒ†ãƒ¼ãƒ:** {keyword}\n"
    report += f"**ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒª:** {', '.join(queries)}\n\n"
    report += f"{len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n\n"

    for i, (paper, analysis) in enumerate(zip(papers, analyses)):
        report += f"## {i+1}. {paper['title']}\n"
        report += f"- **URL:** {paper['url']}\n"
        report += f"### æ ¸å¿ƒçš„ãªè²¢çŒ®\n{analysis}\n"
        report += f"### è¦ç´„ (Abstract)\n{paper['summary']}\n"
        report += "---\n"

    return {**state, "report_markdown": report.strip()}


def get_agent():
    builder = StateGraph(AgentState)

    builder.add_node("generate_queries", generate_queries)
    builder.add_node("find_core_papers", find_core_papers)
    builder.add_node("analyze_paper", analyze_paper_with_llm)
    builder.add_node("compile_report", compile_report)

    builder.add_edge(START, "generate_queries")
    builder.add_edge("generate_queries", "find_core_papers")
    builder.add_edge("find_core_papers", "analyze_paper")
    builder.add_edge("analyze_paper", "compile_report")
    builder.add_edge("compile_report", END)

    return builder.compile()


if __name__ == "__main__":
    pass

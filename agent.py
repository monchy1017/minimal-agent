import os
from typing import TypedDict, List, Optional
from dotenv import load_dotenv
import arxiv
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END, START
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

load_dotenv()
if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("OPENAI_API_KEYãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")


@tool
def web_search(query: str) -> str:
    """
    è«–æ–‡å†…ã®ä¸æ˜ãªå°‚é–€ç”¨èªã‚„ã€GitHubã®å®Ÿè£…ãƒªãƒã‚¸ãƒˆãƒªã‚’æ¢ã™ãŸã‚ã«Webæ¤œç´¢ã‚’è¡Œã†
    Googleæ¤œç´¢ã®ä»£ã‚ã‚Šã«ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print(f"\n  ğŸ” [Toolä½¿ç”¨] Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­: '{query}'")
    try:
        search = DuckDuckGoSearchRun()
        result = search.invoke(query)
        # çµæœã®é•·ã•ã‚‚è¡¨ç¤ºã—ã¦ã¿ã‚‹
        print(f"     -> çµæœå–å¾—æˆåŠŸ ({len(result)}æ–‡å­—)")
        return result
    except Exception as e:
        print(f"     -> æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"


class PaperInfo(TypedDict):
    title: str
    summary: str
    url: str


class AgentState(TypedDict):
    keyword: str
    queries: List[str]  # [NEW] ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ
    core_papers: Optional[List[PaperInfo]]
    analysis: Optional[List[str]]
    web_search_logs: Optional[List[str]]
    report_markdown: Optional[str]


def generate_queries(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰0: [NEW] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’arXivç”¨ã®è‹±èªã‚¯ã‚¨ãƒªã«å¤‰æ›ã™ã‚‹"""
    print("generating search queries...")
    keyword = state["keyword"]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = f"""
    You are an expert AI researcher.
    Please generate 3 effective search queries for arXiv based on the user's input topic.
    
    Requirements:
    1. Translate to English if necessary.
    2. Use specific academic terms (e.g., "LLM efficiency" -> "Model Compression", "Quantization").
    3. Output ONLY the 3 queries, separated by newlines. No numbering or bullets.
    
    User Input: "{keyword}"
    """

    response = llm.invoke([HumanMessage(content=prompt)])

    # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ãƒªã‚¹ãƒˆåŒ–ã—ã€ç©ºç™½ã‚’é™¤å»
    queries = [q.strip() for q in response.content.split("\n") if q.strip()]

    print(f"Generated Queries: {queries}")
    return {**state, "queries": queries}


def find_core_papers(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰1: ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ä½¿ã£ã¦è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹"""
    print("finding core papers...")

    # ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªãŒãªã„å ´åˆã¯å…ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    queries = state.get("queries", [state["keyword"]])

    client = arxiv.Client()
    all_papers: List[PaperInfo] = []
    seen_urls = set()

    # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã‚’å®Ÿè¡Œ
    for query in queries:
        print(f"  Search arXiv with: '{query}'")
        search = arxiv.Search(
            query=query,
            max_results=3,
            sort_by=arxiv.SortCriterion.Relevance,
        )

        try:
            for result in client.results(search):
                if result.pdf_url not in seen_urls:
                    paper = PaperInfo(
                        title=result.title,
                        summary=result.summary,
                        url=result.pdf_url,
                    )
                    all_papers.append(paper)
                    seen_urls.add(result.pdf_url)
        except Exception as e:
            print(f"  Error searching for '{query}': {e}")
    final_papers = all_papers[:10]

    if not final_papers:
        raise ValueError(f"è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Queries: {queries}")

    print(f"Total {len(final_papers)} unique papers found.")
    return {**state, "core_papers": final_papers}


def analyze_paper_with_llm(state: AgentState) -> AgentState:
    """
    ãƒãƒ¼ãƒ‰2: LLMã§è«–æ–‡ã‚’åˆ†æã™ã‚‹
    å¿…è¦ãªã‚‰æ¤œç´¢ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
    """
    print("analyzing papers with llm...")
    papers = state.get("core_papers")
    if not papers:
        return state
    tools = [web_search]
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0).bind_tools(tools)
    agent_executor = create_react_agent(llm, tools)
    analysis_results: List[str] = []
    all_web_search_logs: List[str] = []
    for i, paper in enumerate(papers):
        print(f"Analyzing ({i+1}/{len(papers)}): {paper['title'][:30]}...")
        prompt = f"""
        You are a thorough researcher.
        Read the abstract below and summarize the "Core Contribution" in Japanese (about 300 characters).
        
        If the summary contains â€œunknown technical termsâ€ or â€œimplementation statusâ€
        requiring additional information, please use the provided [web_search] tool to investigate.
        Once sufficient information has been gathered, please output the final explanation.
            
        Title: {paper['title']}
        Abstract:
        {paper['summary']}
        
        Core Contribution (Japanese):
        """
        try:
            result = agent_executor.invoke(
                {"messages": [HumanMessage(content=prompt)]}
            )
            messages = result["messages"]
            for msg in messages:
                if isinstance(msg, AIMessage) and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if tool_call["name"] == "web_search":
                            query = tool_call["args"].get("query")
                            log_entry = (
                                f"{query} (Context: {paper['title'][:20]}...)"
                            )
                            all_web_search_logs.append(log_entry)
            final_response = messages[-1].content
            analysis_results.append(final_response)
        except Exception as e:
            print(f"Error analyzing paper {i}: {e}")
            analysis_results.append("åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    return {
        **state,
        "analysis": analysis_results,
        "web_search_logs": all_web_search_logs,
    }


def compile_report(state: AgentState) -> AgentState:
    """ãƒãƒ¼ãƒ‰3: ãƒ¬ãƒãƒ¼ãƒˆä½œæˆï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªã®è¨˜éŒ²ã‚’è¿½åŠ ï¼‰"""
    print("compiling report...")

    papers = state.get("core_papers")
    analyses = state.get("analysis")
    keyword = state["keyword"]
    queries = state.get("queries", [])
    web_logs = state.get("web_search_logs", [])

    if not papers or not analyses:
        return {**state, "report_markdown": "æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}
    report = f"# è«–æ–‡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {keyword}\n\n"

    report += "## ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª\n"
    if queries:
        for q in queries:
            report += f"- `{q}`\n"
    else:
        report += f"- `{keyword}` (ã‚¯ã‚¨ãƒªå¤‰æ›ãªã—)\n"

    if web_logs:
        report += "## ğŸŒ å®Ÿè¡Œã•ã‚ŒãŸWebæ¤œç´¢ (DuckDuckGo)\n"
        for log in web_logs:
            report += f"- `{log}`\n"
        report += "\n"

    report += f"\n**ãƒ’ãƒƒãƒˆä»¶æ•°:** {len(papers)}ä»¶\n\n"
    report += "---\n\n"

    for i, (paper, analysis) in enumerate(zip(papers, analyses)):
        report += f"## {i+1}. {paper['title']}\n"
        report += f"**URL:** {paper['url']}\n\n"
        report += f"{analysis}\n\n"
        # report += f"### è¦ç´„ (Abstract)\n{paper['summary']}\n"
        report += "---\n"

    return {**state, "report_markdown": report.strip()}


def get_agent():
    builder = StateGraph(AgentState)

    builder.add_node("generate_queries", generate_queries)
    builder.add_node("find_core_papers", find_core_papers)
    builder.add_node("analyze_paper", analyze_paper_with_llm)
    builder.add_node("compile_report", compile_report)

    builder.add_edge(START, "generate_queries")
    builder.add_edge("generate_queries", "find_core_papers")
    builder.add_edge("find_core_papers", "analyze_paper")
    builder.add_edge("analyze_paper", "compile_report")
    builder.add_edge("compile_report", END)

    return builder.compile()


if __name__ == "__main__":
    pass
